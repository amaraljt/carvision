{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# notebook playground for bigger dataset\n",
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "import albumentations as A\n",
    "from torchvision.io import read_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    A class representing an employee.\\n \\n    Attributes:\\n        name (str): The name of the employee.\\n        age (int): The age of the employee.\\n        department (str): The department the employee works in.\\n        salary (float): The salary of the employee.\\n    '"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# google docstring template\n",
    "\"\"\"\n",
    "    A class representing an employee.\n",
    " \n",
    "    Attributes:\n",
    "        name (str): The name of the employee.\n",
    "        age (int): The age of the employee.\n",
    "        department (str): The department the employee works in.\n",
    "        salary (float): The salary of the employee.\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes_labels = ['bicycle', 'bus', 'car', 'motorbike', 'person']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VtecDataset():\n",
    "    \"\"\"\n",
    "    Traffic Detection Dataset\n",
    "        <class_ID> <x_center> <y_center> <width> <height>\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, img_dir, label_dir, transform=None, target_transform=None):\n",
    "        \"\"\"\n",
    "        Attributes:\n",
    "            img_dir (string): Path to training image directory\n",
    "            label_dir (string): Path to training label directory\n",
    "            transform (callable, optional): Optional transform to be applied on a sample\n",
    "        \"\"\"\n",
    "        self.img_dir = img_dir\n",
    "        self.label_dir = label_dir\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.images = [f for f in os.listdir(img_dir) if f.endswith('.jpg')]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        \"\"\"Returns subset of data and targets corresponding to given indices.\"\"\"\n",
    "        image_name = self.images[i]\n",
    "        image_path = os.path.join(self.img_dir, image_name)\n",
    "        image = read_image(image_path)\n",
    "        \n",
    "        label_name = image_name.replace('.jpg', '.txt')\n",
    "        label_path = os.path.join(self.label_dir, label_name)\n",
    "        labels = self.get_labels(label_path)\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.target_transform:\n",
    "            labels = self.target_transform(labels)\n",
    "        \n",
    "        return image, labels\n",
    "    \n",
    "    \n",
    "    def get_labels(self, label_path):\n",
    "        \"\"\"\n",
    "        Loads labels for the corresponding image and returns a dictionary\n",
    "        with keys \"boxes\" and \"labels\" \n",
    "        \"\"\"\n",
    "        labels = {\"boxes\": [], \"labels\": []}\n",
    "        with open(label_path, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "        \n",
    "            for line in lines:\n",
    "                class_id, x_center, y_center, width, height = map(float, line.strip().split())\n",
    "                labels[\"boxes\"].append([x_center, y_center, width, height])\n",
    "                labels[\"labels\"].append(class_id)\n",
    "            \n",
    "        labels[\"boxes\"] = torch.tensor(labels[\"boxes\"], dtype=torch.float32)\n",
    "        labels[\"labels\"] =torch.tensor(labels[\"labels\"], dtype=torch.long)\n",
    "            \n",
    "        return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_img_dir = \"../data/DetectionDataset/train/images/\"\n",
    "train_label_dir = \"../data/DetectionDataset/train/labels/\"\n",
    "test_img_dir = \"../data/DetectionDataset/test/images/\"\n",
    "test_label_dir = \"../data/DetectionDataset/test/images/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    \"\"\"Handles variable label sizes : stacks images instead of labels\"\"\"\n",
    "    \n",
    "    images = []\n",
    "    boxes = []\n",
    "    labels = []\n",
    "        \n",
    "    for sample in batch:\n",
    "        image, target = sample\n",
    "        images.append(image)\n",
    "        boxes.append(target[\"boxes\"])\n",
    "        labels.append(target[\"labels\"])\n",
    "\n",
    "    images = torch.stack(images, dim=0)\n",
    "    \n",
    "    return images, {\"boxes\": boxes, \"labels\": labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transforms = transforms.Compose([transforms.Normalize(0.1, 0.5, 0.2)])\n",
    "\n",
    "train_dataset = VtecDataset(train_img_dir, train_label_dir)\n",
    "train_dataloader = DataLoader(train_dataset, 4, collate_fn=collate_fn, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features, train_labels = next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 64,  50,  43,  ...,  88, 100, 106],\n",
       "          [ 64,  51,  44,  ...,  89,  95,  99],\n",
       "          [ 64,  53,  46,  ...,  80,  69,  65],\n",
       "          ...,\n",
       "          [206, 189, 199,  ..., 139, 124, 127],\n",
       "          [216, 203, 215,  ..., 134, 119, 121],\n",
       "          [201, 194, 208,  ..., 139, 123, 107]],\n",
       "\n",
       "         [[111,  97,  92,  ...,  88, 100, 106],\n",
       "          [111,  98,  93,  ...,  89,  95,  99],\n",
       "          [111, 100,  95,  ...,  80,  69,  65],\n",
       "          ...,\n",
       "          [207, 190, 200,  ..., 140, 125, 128],\n",
       "          [217, 204, 216,  ..., 135, 120, 122],\n",
       "          [202, 195, 209,  ..., 140, 124, 108]],\n",
       "\n",
       "         [[119, 105,  97,  ...,  88, 100, 106],\n",
       "          [119, 106,  98,  ...,  89,  95,  99],\n",
       "          [119, 108, 100,  ...,  80,  69,  65],\n",
       "          ...,\n",
       "          [191, 174, 186,  ..., 144, 129, 132],\n",
       "          [201, 188, 202,  ..., 139, 124, 126],\n",
       "          [186, 179, 195,  ..., 144, 128, 112]]],\n",
       "\n",
       "\n",
       "        [[[ 37,  43,  49,  ...,  58,  85,  78],\n",
       "          [ 41,  45,  49,  ...,   0,  59,  87],\n",
       "          [ 45,  46,  47,  ...,  45,  51,  35],\n",
       "          ...,\n",
       "          [ 32,  43,  57,  ...,  66,  68,  68],\n",
       "          [ 46,  46,  50,  ...,  69,  67,  66],\n",
       "          [ 57,  49,  46,  ...,  69,  65,  62]],\n",
       "\n",
       "         [[ 40,  46,  52,  ...,  71,  97,  90],\n",
       "          [ 44,  48,  52,  ...,   2,  71,  99],\n",
       "          [ 48,  49,  50,  ...,  58,  64,  48],\n",
       "          ...,\n",
       "          [ 31,  42,  56,  ...,  62,  64,  64],\n",
       "          [ 45,  45,  50,  ...,  65,  63,  62],\n",
       "          [ 56,  48,  46,  ...,  65,  61,  58]],\n",
       "\n",
       "         [[ 49,  55,  59,  ...,  90, 119, 112],\n",
       "          [ 53,  57,  59,  ...,  21,  93, 121],\n",
       "          [ 57,  58,  57,  ...,  77,  83,  67],\n",
       "          ...,\n",
       "          [ 26,  37,  51,  ...,  50,  53,  53],\n",
       "          [ 40,  40,  42,  ...,  53,  52,  51],\n",
       "          [ 51,  43,  38,  ...,  53,  50,  47]]],\n",
       "\n",
       "\n",
       "        [[[119, 119, 105,  ..., 184, 183, 182],\n",
       "          [122, 122, 111,  ..., 184, 183, 183],\n",
       "          [127, 112, 126,  ..., 185, 184, 184],\n",
       "          ...,\n",
       "          [ 91,  82,  89,  ...,  87,  87,  87],\n",
       "          [ 98,  89,  95,  ...,  87,  87,  87],\n",
       "          [ 96,  97, 100,  ...,  87,  87,  87]],\n",
       "\n",
       "         [[121, 120, 108,  ..., 210, 209, 208],\n",
       "          [124, 123, 114,  ..., 210, 209, 209],\n",
       "          [129, 113, 129,  ..., 211, 210, 210],\n",
       "          ...,\n",
       "          [ 97,  88,  95,  ...,  90,  90,  90],\n",
       "          [104,  95, 101,  ...,  90,  90,  90],\n",
       "          [102, 103, 106,  ...,  90,  90,  90]],\n",
       "\n",
       "         [[110, 112, 101,  ..., 237, 236, 235],\n",
       "          [113, 115, 107,  ..., 237, 236, 236],\n",
       "          [118, 105, 122,  ..., 238, 237, 237],\n",
       "          ...,\n",
       "          [113, 104, 111,  ..., 107, 107, 107],\n",
       "          [120, 111, 117,  ..., 107, 107, 107],\n",
       "          [118, 119, 122,  ..., 107, 107, 107]]],\n",
       "\n",
       "\n",
       "        [[[102,  99,  96,  ...,  85,  87, 101],\n",
       "          [ 58,  57,  55,  ...,  32,  37,  62],\n",
       "          [ 54,  56,  57,  ...,  25,  22,  50],\n",
       "          ...,\n",
       "          [ 64,  83,  87,  ...,  76,  86,  62],\n",
       "          [ 63,  81,  85,  ...,  79,  84,  62],\n",
       "          [ 59,  77,  80,  ...,  82,  84,  64]],\n",
       "\n",
       "         [[105, 102,  99,  ..., 117, 112, 124],\n",
       "          [ 61,  60,  58,  ...,  64,  64,  85],\n",
       "          [ 57,  59,  60,  ...,  56,  49,  76],\n",
       "          ...,\n",
       "          [ 68,  87,  91,  ...,  70,  80,  56],\n",
       "          [ 67,  85,  89,  ...,  71,  76,  54],\n",
       "          [ 63,  81,  84,  ...,  74,  76,  56]],\n",
       "\n",
       "         [[110, 107, 104,  ..., 142, 132, 140],\n",
       "          [ 66,  65,  63,  ...,  89,  83, 101],\n",
       "          [ 62,  64,  65,  ...,  84,  70,  93],\n",
       "          ...,\n",
       "          [ 77,  96, 100,  ...,  84,  94,  70],\n",
       "          [ 76,  94,  98,  ...,  86,  91,  69],\n",
       "          [ 72,  90,  93,  ...,  89,  91,  71]]]], dtype=torch.uint8)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'boxes': [tensor([[0.7516, 0.5844, 0.1922, 0.3422],\n",
       "          [0.4234, 0.2328, 0.1172, 0.2313],\n",
       "          [0.2875, 0.7305, 0.1016, 0.1250]]),\n",
       "  tensor([[0.3727, 0.7141, 0.0297, 0.1469],\n",
       "          [0.1336, 0.4844, 0.0219, 0.0969],\n",
       "          [0.6586, 0.8211, 0.1281, 0.2438],\n",
       "          [0.7656, 0.6156, 0.2219, 0.3953],\n",
       "          [0.5086, 0.7070, 0.0312, 0.1594],\n",
       "          [0.5641, 0.6672, 0.0281, 0.1484],\n",
       "          [0.5672, 0.5664, 0.0312, 0.1344],\n",
       "          [0.6203, 0.3977, 0.0625, 0.1078],\n",
       "          [0.5227, 0.4078, 0.0609, 0.1172],\n",
       "          [0.5406, 0.2852, 0.0516, 0.0891],\n",
       "          [0.5047, 0.2406, 0.0516, 0.0766],\n",
       "          [0.4617, 0.2070, 0.0125, 0.0531],\n",
       "          [0.6133, 0.2523, 0.0531, 0.0875],\n",
       "          [0.5891, 0.1922, 0.0391, 0.0625],\n",
       "          [0.6445, 0.1492, 0.0344, 0.0625],\n",
       "          [0.6672, 0.0977, 0.0281, 0.0453],\n",
       "          [0.6344, 0.0805, 0.0266, 0.0391],\n",
       "          [0.6492, 0.0445, 0.0266, 0.0359],\n",
       "          [0.6406, 0.0203, 0.0203, 0.0312],\n",
       "          [0.6852, 0.0492, 0.0250, 0.0359],\n",
       "          [0.8109, 0.1023, 0.0266, 0.0391],\n",
       "          [0.8820, 0.2258, 0.0078, 0.0516],\n",
       "          [0.9461, 0.2992, 0.0125, 0.0625]]),\n",
       "  tensor([[0.5758, 0.3773, 0.0672, 0.0609],\n",
       "          [0.9359, 0.3984, 0.0375, 0.0437],\n",
       "          [0.3500, 0.3812, 0.0391, 0.0641],\n",
       "          [0.4336, 0.3578, 0.0375, 0.0578],\n",
       "          [0.1719, 0.4648, 0.0141, 0.0641],\n",
       "          [0.3891, 0.3727, 0.0406, 0.0641],\n",
       "          [0.3109, 0.3766, 0.0359, 0.0578],\n",
       "          [0.3008, 0.3391, 0.0281, 0.0422],\n",
       "          [0.2117, 0.4547, 0.0172, 0.0750],\n",
       "          [0.9898, 0.4922, 0.0172, 0.0562],\n",
       "          [0.4109, 0.3289, 0.0328, 0.0578],\n",
       "          [0.1227, 0.3344, 0.0328, 0.0547],\n",
       "          [0.9867, 0.5055, 0.0172, 0.0531],\n",
       "          [0.3422, 0.3375, 0.0312, 0.0312],\n",
       "          [0.2047, 0.4461, 0.0156, 0.0594],\n",
       "          [0.5523, 0.4922, 0.4328, 0.2484]]),\n",
       "  tensor([[0.4742, 0.3258, 0.0750, 0.1297],\n",
       "          [0.5750, 0.3383, 0.0750, 0.1797],\n",
       "          [0.2562, 0.2695, 0.0875, 0.0844],\n",
       "          [0.2180, 0.3781, 0.1391, 0.1516],\n",
       "          [0.3703, 0.2062, 0.0531, 0.0656]])],\n",
       " 'labels': [tensor([2, 2, 0]),\n",
       "  tensor([3, 3, 2, 4, 3, 3, 3, 2, 2, 2, 2, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3]),\n",
       "  tensor([2, 2, 2, 2, 4, 2, 2, 2, 4, 4, 2, 2, 4, 2, 4, 1]),\n",
       "  tensor([2, 2, 2, 2, 2])]}"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
